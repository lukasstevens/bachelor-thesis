% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\newcommand{\apxalg}{\mathcal{A}}
\newcommand{\oset}[2]{%
    \tikz[baseline=(X.base),inner sep=0pt,outer sep=0pt]{%
        \node[inner sep=0pt,outer sep=0pt] (X) {$#2$}; 
            \node[yshift=1pt] at (X.north) {$#1$};
    }
}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\tildaS}{\oset{\ \scriptstyle\sim}{\calS}}

\chapter{Theoretischer Hintergrund}\label{chapter:theorie}
\section{Berechnungskomplexität von perfekt ausgewogenen Partitionierungen \todo{Titel!}}
Wie bereits im vorherigem Kapitel geschildert, ist das $(2,1)$-Partitionierungsproblem, auch \todo{Minimum-Bisection erklären?} Minimum-Bisection-Problem genannt, ein wohlbekanntes Thema in der Forschung. 
Es ist bereits länger bekannt, dass es sich beim Minimum-Bisection-Problem um ein NP-vollständiges Problem handelt~\parencite{gj79}.
In jüngerer Vergangenheit wurde von Feige und Krauthgamer ein polynomieller Algorithmus präsentiert, der das Problem mit einem polylogarithmischen Approximationsfaktor löst. \parencite{fk02}
Wie sich herausstellte, lässt sich dieses Ergebnis jedoch nicht auf auf das $(k,1)$-Partitionierungsproblem übertragen.
Es kann nämlich keinen polynomiellen Approximationsalgorithmus für das $(k,1)$-Partitionierungsproblem, wobei $k$ keine Konstante ist, geben.
Diese Behautpung wird im folgenden Satz, der aus \parencite{ar06} entnommen ist, gezeigt. \\

\begin{thm}\label{thm:np_comp}
    Vorausgesetzt $P \neq NP$, dann gibt es für das $(k,1)$-Partitionierungsproblem keinen polynomiellen Approximationsalgorithmus mit endlichem Approximationsfaktor.
\end{thm}
\begin{proof}
    Um die Aussage zu zeigen, reduzieren wir das $3$-Partitionierungsproblem auf das $(k,1)$-Partitionierungsproblem. 
    Das $3$-Partitionierungsproblem ist folgendermaßen definiert: Gegeben $n = 3k$ ganze Zahlen $a_1,\ldots, a_n$ und einen Schwellwert $A$, sodass $\frac{A}{4} < a_i < \frac{A}{2}$ für alle $i \in [n]$ gilt, und 
    \begin{equation*}
        \sum_{i=1}^{n} a_i = kA.
    \end{equation*}
    Nun soll entschieden werden, ob eine Partitionierung der Zahlen $a_1, \ldots, a_n$ in $k$ Tripel, die alle zu $A$ summieren, möglich ist. 
    In \parencite{gj79} wurde die starke NP-Vollständigkeit dieses Problems gezeigt, das heißt, es ist auch schon NP-vollständig, wenn sowohl alle $a_i$, als auch $A$, polynomiell beschränkt sind.
    Seien die $a_i$ und $A$ im Folgenden polynomiell beschränkt.

    \noindent Wir nehmen zum Widerspruch an, dass wir einen Approximationsalgorithmus $\apxalg$ mit endlichem Approximationsfaktor für das $(k,1)$-Partitionierungsproblem haben. 
    Wir zeigen nun, dass wir $\apxalg$ benutzen können, um eine Instanz des $3$-Partitionierungsproblems zu lösen.
    Dazu konstruieren wir einen Graphen $G$, in dem es für jede der gegebenen Zahlen $a_i$ ein Clique mit $a_i$ Knoten gibt.
	Man bemerke dabei, dass $G$ in polynomieller Zeit konstruiert werden kann, da alle $a_i$ und $A$ polynomiell beschränkt sind.
    Wenn es eine Lösung für die Instanz des $3$-Partitionierungsproblems gibt, dann findet $\apxalg$ eine Lösung die keine Kanten in $G$ schneidet. 
    Auf der anderen Seite schneidet $\apxalg$ mindestens eine Kante in $G$, wenn keine Lösung existiert.
    Mit $\apxalg$ kann also entschieden werden, ob eine Lösung existiert, und somit kann das $3$-Partitionierungsproblem gelöst werden.
    Dies ist jedoch ein Widerspruch zur Annahme, dass es unter der Voraussetzung $P \neq NP$ keinen Algorithmus gibt, der das $3$-Partitionierungsproblem in polynomieller Zeit löst.
\end{proof}

Weiterhin wurde dieses Ergebnis in \parencite{ff13} verfeinert. 
Dort wurde gezeigt, dass das $(k,1)$-Partitionierungsproblem auch dann NP-vollständig bleibt, wenn man sich auf Bäume als Eingabegraphen beschränkt.
Insbesondere wurden ungewichtete Bäume mit Maximaldurchmesser $4$, das heißt, die Länge des längsten Pfades zwischen zwei beliebigen Blättern ist höchstens $4$, betrachtet.
Schon für diese Bäume kann für das $(k,1)$-Partitionierungsproblem kein Approximationsalgorithmus mit Approximationsfaktor $n^c$ für beliebige Konstanten $c < 1$ gefunden werden.
Auch für ungewichtete Bäume mit maximalen Knotengrad $5$ kann es keinen polynomiellen Algorithmus geben.

Die vorangegangenen Erkenntisse zeigen, dass auch für bestimmte eingeschränkte Graphen kein Approximationsalgorithmus für das $(k,1)$-Partitionierungsproblem gefunden werden kann, außer es gilt $P=NP$. 
Deshalb liegt die Verwendung eines bikriteriellen Algorithmus nahe, also ein Algorithmus, der die Anforderung relaxiert, dass alle $k$ Teile die gleiche Größe haben.
Dabei wird, anstatt das $(k,1)$-Partitionierungsproblem zu lösen, stattdessen das $(k,1+\eps)$-Partitionierungsproblem mit $\eps > 0$ betrachtet.
Für dieses Problem wurde von Andreev und Räcke ein polynomieller Algorithmus mit Approximationsfaktor $\bigO(1/\eps^2 \cdot \log^{1.5} n)$ für konstantes $\eps > 0$ präsentiert. \parencite{ar06} 

\section{Ein bikriterieller Graphpartitionerungsalgorithmus}
Wie im vorherigem Abschnitt gezeigt, ist das $(k,1)$-Partitionierungsproblems schwer zu approximieren. 
Deshalb wurde ein bikriterieller Ansatz nahegelegt, das bedeutet man approximiert in Hinsicht auf die Ausgeglichenheit der Partitionenierung und in Hinsicht auf die Schnittkosten.
In diesem Abschnitt wird ein polynomieller bikriterieller Approximationsalgorithmus mit Approximationsfaktor $\bigO(\log\, n)$, wie in \parencite{ff13} beschrieben, vorgestellt.
Dieser Algorithmus arbeitet ausschließlich auf Bäumen, er kann jedoch auf generelle Graphen erweitert werden, indem man den Eingabegraphen vorher in einen Baum mit ähnlichen Schnittkosten umwandelt.
Für diese Umwandlung wird ein Verfahren verwendet, das von Räcke entwickelt wurde. \parencite{rc08}

\subsection{Fast ausgewogene Partitionierung von Bäumen}
In dieser Sektion wird das $(k,1+\eps)$-Partitionierungsproblem auf Bäumen mit Kantengewichten untersucht. 
Die folgenden Ergebnisse werden zeigen, dass es für Bäume mit Kantengewichten einen polynomiellen Algorithmus gibt, der das Problem löst und ferner eine Lösung nicht schlechter als die Optimallösung des $(k,1)$-Partitionierungsproblems im Bezug auf die Schnittkosten liefert.
Damit ist dieser Algorithmus ein \todo{PTAS einführen} PTAS für das $(k,1)$-Partitionierungsproblem auf Bäumen. 
Gegeben sei ein Baum $T = (V,E,\weight)$ mit $n$ Knoten, wobei $\weight : E \rightarrow \reals^+$ die Gewichtsfunktion ist.
Prinzipiell löst das folgende Verfahren das $(k,1)$-Partitionierungsproblem in zwei Schritten.
Im ersten Schritt werden alle Möglichkeiten berechnet den Baum $T$ in Zusammenhangskomponenten zu zerlegen.
Diese Zusammenhangskomponenten werden der Größe nach gruppiert.
Dann kann eine Partitionierung von $T$ durch eine Menge von disjunkten Zusammenhangskomponenten dargestellt werden, die zusammen $T$ ergeben. 
Diese Mengen von Zusammenhangskomponenten werden nun anhand der Größen der enthaltenen Komponenten in Äquivalenzklassen eingeordnet, wobei zwei Mengen genau dann äquivalent sind, wenn sie dieselbe Anzahl an Komponenten der Größe $x$ für alle $x \in \{1, \ldots, \ceil*{n/k}\}$ enthalten.
Für jede Äquivalenzklasse $\calS$ wird die Partitionierung mit den geringsten Schnittkosten als Repräsentant für $\calS$ gewählt.
Im zweiten Schritt werden dann nur die Äquivalenzklassen betrachtet, deren enthaltene Zusammenhangskomponenten sich in $k$ Mengen mit Größe höchstens $\ceil*{n/k}$ packen lassen und somit eine gültige Partitionierung darstellen.
Anschließend wird als Lösung der Repräsentant der übrigen Äquivalenzklasse ausgewählt, der die niedrigsten Schnittkosten verursacht.
Offensichtlich kann mit diesem Verfahren eine optimale Lösung für das $(k,1)$-Partitionierungsproblem gefunden werden.
Die Laufzeit dieses Verfahrens ist jedoch exponentiell in der Anzahl der Knoten $n$, da insbesondere die Anzahl der Äquivalenzklassen exponentiell in $n$ ist.

Um eine polynomieller Laufzeit zu erreichen, werden die Zusammenhangskomponenten in gröbere Äquivalenzklassen eingeordnet.
Anstatt Komponenten nach der genauen Größe zu gruppieren, werden die möglichen Größen in Intervalle aufgeteilt, das heißt, Komponenten mit verschiedenen Größen werden entsprechend in Intervallen zusammengefasst. 
Wie oben gehören zwei Mengen zur selben Äquivalenzklasse, wenn sie für jedes der Intervalle gleich viele Komponenten in diesem Größenintervall haben.
Durch eine geschickte Wahl der Intervalle in Abhängigkeit von $\eps$ kann die Anzahl der Äquivalenzklassen für konstantes $\eps$ auf eine polynomielle Anzahl im Bezug auf $n$ beschränkt werden.
Diese groben Äquivalenzklassen verursachen jedoch einen Approximationsfehler und man erhält keine perfekt ausgewogene Partitionierung mehr sondern eine $(k, 1+\eps)$-Partitionierung. Die folgende Definition legt die Wahl der Intervalle fest. \\

\begin{defn}[Signatur]\label{defn:signature}
    Sei $\calS$ eine Menge von Zusammenhangskomponenten über der Knotenmenge $V$ von $T$ und $\eps > 0$. Ein Vektor $\vec{g} = (g_0, \ldots, g_t)$ mit $t = \ceil*{\log_{1+\eps}(1/e)} + 1$ wird Signatur von $\calS$ genannt, wenn $\calS$ genau $g_0$ Komponenten mit Größe in $[1,\, \eps \ceil*{n/k})$ und $g_i$ Komponenten mit Größe in $[{(1+\eps)}^{i-1}\, \cdot\, \eps \ceil*{n/k},\, {(1+\eps)}^i\, \cdot \, \eps \ceil*{n/k})$ für $i \in \{1, \ldots, t\}$ enthält.
\end{defn}

In einer groben Äquivalenzklasse befinden sich alle Mengen von Zusammenhangskomponenten, die dieselbe Signatur haben.
Der Repräsentant ist also die Menge von Zusammenhangskomponenten, die unter allen Mengen mit der gleichen Signatur die geringsten Schnittkosten aufweist.
Im ersten Schritt verwendet man dynamische Programmierung, um die Signaturen und die zugehörigen Repräsentanten zu finden.
Dabei verwendet einen Ansatz, der eine Verallgemeinerung des Ansatzes für das Bisektionsproblem auf Bäumen ist. \parencite{mcg78, ws11}
Die dynamische Programmierung findet für jede Signatur einen Repräsentanten mit optimalen Schnittkosten, der alle Knoten enthält.
Sei nun $\bbS$ die Menge dieser Repräsentanten.
Im zweiten Schritt versucht der Algorithmus für jeden Repräsentanten $\calS \in \bbS$ die enthaltenen Zusammenhangskomponenten in Behälter mit Kapazität $(1+\eps)\ceil*{n/k}$ zu packen. 
Hierbei handelt es sich um das Bin-Packing-Problem, wofür von Hochbaum und Shmoys in \parencite{hs86} ein Approximationsalgorithmus vorgestellt wurde.
Die schlussendliche Ausgabe des Algorithmus ist eine Partitionierung des Graphen gegeben durch das in der Bin-Packing-Instanz gefundene Packing.
Für die Ausgabe wird unter den Repräsentanten, deren Komponenten sich in höchstens $k$ Behälter packen lassen, der Repräsentant mit den geringsten Schnittkosten ausgewählt.
Beide Schritte des Algorithmus haben für konstantes $\eps$ eine polynomielle Laufzeit.

\subsection{Die Schnittphase}
Im Folgenden wird beschrieben, wie man mit Hilfe von dynamischer Programmierung für alle möglichen Signaturen $\vec{g}$ den Repräsentanten berechnet, also die Menge von Zusammenhangskomponenten, die optimale Schnittkosten für eine bestimmte Signatur $\vec{g}$ erreicht.
Dieses Verfahren wurde zuerst von Feldmann und Foschini vorgestellt~\parencite{ff13} und ist wiederum eine Verallgemeinerung der Lösungsmethode für das Bisektionsproblem auf Bäumen. \parencite{mcg78, ws11}
Zuerst wird eine Wurzel $r \in V$ gewählt, wobei $V$ die Knotenmenge von $T$ ist. 
Angefangen an der Wurzel $r$ wird dann eine Ordnung der Kinder für jeden Knoten $v \in V$ festgelegt, wodurch sich für jeden Knoten genau ein Elternknoten ergibt.
Durch diese Ordnung können wir für einen Knoten $v$ von seinem linkesten und seinem rechtesten Kindknoten sprechen. 
Zusätzlich ergeben sich aus dieser Ordnung auch die Bezeichnung der linken Geschwisterknoten von $v$ und des Vorgängers von $v$ unter diesen linken Geschwisterknoten.
Intuitiv können die Mengen der disjunkten Zusammenhangskomponente an einem Knoten $v \neq r$ berechnen werden, indem rekursiv die optimalen Lösungen der Teilbäume, die in den Geschwisterknoten und den Kindknoten von $v$ gewurzelt sind, kombiniert werden.
Diese Idee soll nun weiter formalisiert werden.

Für $v \neq r$ definiert sich die Menge $L_v \subset V$ durch die Vereinigung der Knotenmengen von allen Teilbäumen gewurzelt in den Geschisterknoten von $v$ und $v$ selbst.
Außerdem wird eine Menge von disjunkten Zusammenhangskomponenten $\mathcal{F}$ dann als eine untere Grenze von $L_v$ bezeichnet, wenn alle Knoten in $\mathcal{F}$ auch in $L_v$ sind und die übrigen Knoten $V \setminus \mathcal{F}$ eine Zusammenhangskomponente mit der Wurzel $r$ bilden.
Wenn man nun an einem Knoten $v \neq r$ die Signatur $\vec{g}$ hat, dann kann der Algorithmus rekursiv die untere Grenze $\mathcal{F}$ zu $L_v$ mit dieser Signatur $\vec{g}$ finden.
Im letzten Schritt können für alle Signaturen $\vec{g}$ Repräsentanten berechnet werden, die die ganze Knotenmenge $V$ abdecken, indem man die Lösungen am rechtesten Kind der Wurzel $r$ verwendet.
Dabei verwendet der Algorithmus in jedem Rekursionsschritt die Menge von Zusammenhangskomponenten mit den geringsten Kosten. 
Um diese Kosten zu messen, wird eine Kostenfunktion $C_v(\vec{g}, m)$ eingeführt, die für einen beliebigen Knoten $v \neq r$ und eine Knotenanzahl $m$ die optimalen Schnittkosten über alle unteren Grenzen von $L_v$ angibt, die Signatur $\vec{g}$ haben und $m$ Knoten überdecken.
Wenn es keine untere Grenze zu $L_v$ mit diesen Parametern gibt, dann sei $C_v(\vec{g}, m) = \infty$.
Zur Abkürzung der Notation definieren wir noch $\mu \coloneqq (1+\eps)\ceil*{n/k}$ und die Signatur $\vec{e}(x)$, welche für einen Knotenanzahl $x < \mu$ die Signatur darstellt, die genau einer Komponente der Größe $x$ enthält.
Nun wird gezeigt, dass sich $C_v(\vec{g}, m)$ durch ein dynamische Programm berechnen lässt.

Sei $\mathcal{F}^*$ die optimale untere Grenze im Bezug auf $C_v(\vec{g}, m)$. Dann ergeben sich vier Fälle, die daraus resultieren, dass ein Knoten entweder kein rechtestes Kind hat, also ein Blatt ist, oder nicht und dass ein Knoten entweder einen linken Geschwisterknoten hat oder nicht.
Nun soll zuerst der Fall betrachtet werden, dass beide Bedingungen eingetreten sind, also der Knoten $v$ ein Blatt ist und keinen linken Geschwisterknoten hat.
Dieser Fall stellt den Rekursionsanker dar, da rekursiv weder die Lösung am rechtesten Kind noch die Lösung am linken Geschwisterknoten verwenden werden kann.
Für diesen Fall gilt $L_v = \{v\}$ und deshalb enthält die Menge $\mathcal{F}^*$ entweder $\{v\}$ als eine Komponente oder sie ist leer.
Wenn letzteres gilt, dann sind die Schnittkosten $0$.
Sonst sind die Schnittkosten $\weight(e)$, wobei $e$ die Kante von $v$ zu seinem Elternknoten ist, da für die untere Grenze $\mathcal{F}^*$ gilt, dass $V \setminus \mathcal{F}^*$ eine eine Komponente darstellt und damit $e$ von der Komponente $\{v\}$ zur Komponente $V \setminus \mathcal{F}^*$ geht.
Also gilt $C_v((0,\ldots,0), 0) = 0$ und $C_v(\vec{e}(1), 1) = \weight(e)$ und alle anderen Funktionswerte sind unendlich.
Der zweite Fall ist, dass $v$ weder ein Blatt ist noch der rechteste Knoten unter seinen Geschwisterknoten. Sei $w$ im Folgenden der Vorgänger von $v$ im Bezug auf die Geschwisterknoten von $v$ und sei $u$ das rechteste Kind von $v$. 
Die Menge $L_v$ ist die Vereinigung der Knotenmengen der Teilbäume, die an $v$ und dessen linken Geschwisterknoten gewurzelt sind.
Jetzt kann $\mathcal{F}^*$ entweder eine untere Grenze sein, wo die Kante $e$ von $v$ zu seinem Elternknoten geschnitten wird oder nicht.
Wenn $e$ nicht geschnitten wird, dann ist $v$ nicht in den $m$ Knoten enthalten, die durch $\mathcal{F}^*$ überdeckt werden, da $v$ dann mit der Komponente, die die Wurzel enthält, verbunden ist.
Damit müssen diese $m$ Knoten in den Mengen $L_w$ und $L_u$ enthalten sein, da gilt $L_v = L_w \setunion L_u \setunion \{v\}$.
Seien nun $x$ der Knoten in $L_u$ von $\mathcal{F}^*$ überdeckt, dann sind $m - x$ der Knoten in $L_w$ von $\mathcal{F}^*$ überdeckt.
Dann muss die Signatur $\vec{g}$ zu $\mathcal{F}^*$ die Summe der Signaturen $\vec{g}_u$ und $\vec{g}_w$ sein, sodass $\vec{g}_u$ (beziehungsweise $\vec{g}_w$) minimale Schnittkosten unter den unteren Grenzen von $L_u$ (beziehungsweise $L_w$), die $x$ Knoten (beziehungsweise $m - x$) Knoten überdecken, hat.
Falls dies nicht der Fall wäre, könnte diese untere Grenze von $L_u$ (beziehungsweise $L_w$) durch eine untere Grenze mit geringeren Kosten ausgetauscht werden, was ein Widerspruch zur Optimalität von $\mathcal{F}^*$ wäre.
Also gilt im Fall, dass $v$ eine rechtestes Kind $u$ hat und $v$ einen Vorgänger $w$ hat und die Kante $e$ von $v$ zu seinem Elternknoten nicht geschnitten wird, dass 
\begin{equation} \label{eq:e_not_cut}
    C_v(\vec{g}, m) = \min \left\{ C_w(\vec{g}_w, m - x) + C_u(\vec{g}_u, x) \mid 0 \leq x \leq m \land \vec{g}_w + \vec{g}_u = \vec{g} \right\}.
\end{equation}

Nun kann es noch sein, dass $v$ kein Blatt ist und $v$ einen linken Geschwisterknoten hat, aber die Kante von $v$ zu seinem Elternknoten nicht geschnitten wird.
Sei $n_v$ die Anzahl der Knoten im Teilbaum gewurzelt in $v$, die durch $\mathcal{F}^*$ überdeckt werden.
Dann müssen die anderen $m - n_v$ Knoten, die durch $\mathcal{F}^*$ überdeckt werden, in $L_w$ enthalten sein.
Weiterhin sei $x$ die Größe der Komponente $S \in \mathcal{F}^*$, die $v$ enthält.
Wie auch oben müssen die unteren Grenzen in $L_u$ und $L_w$ mit den Signaturen $\vec{g}_u$ und $\vec{g}_w$ in $\mathcal{F}^* \ {S}$ minimale Schnittkosten haben.
Also muss die Signatur $\vec{g}$ die Summe von $\vec{g}_u$ und $\vec{g}_w$ und $\vec{e}(x)$ sein.
Damit gilt in diesem Fall
\begin{equation} \label{eq:e_cut}
    \begin{aligned}
        C_v(\vec{g}, m) = \weight(e) + \min \{ & C_w(\vec{g}_w, m - n_v) + C_u(\vec{g}_u, n_v - x) \mid \\ & \qquad 1 \leq x \leq \mu \land \vec{g}_w + \vec{g}_u + \vec{e}(x) = \vec{g} \}.
    \end{aligned}
\end{equation}
Somit kann $C_v(\vec{g}, m)$ für den Fall, dass $v$ weder ein Blatt ist noch keinen linken Geschwisterknoten hat, korrekt berechnet werden, indem man das Minimum der Werte geben durch~\eqref{eq:e_not_cut} und~\eqref{eq:e_cut} bestimmt.
Es bleiben noch zwei Fälle übrig, also wenn $v$ entweder ein Blatt ist oder $v$ der linkeste unter seinen Geschwisterknoten ist.
In diesen Fällen existiert das rechte Kind $u$ oder respektive der Vorgänger $w$ nicht.
Die Definition von $C_v(\cdot, \cdot)$ folgt dann aus den Gleichungen~\eqref{eq:e_not_cut} und~\eqref{eq:e_cut}, indem die Funktionswerte $C_u(\vec{g}, x)$ (respektive $C_w(\vec{g}, x)$ für den nicht existierenden Knoten $u$ (respektive $w$) auf $0$ gesetzt werden, wenn $\vec{g} = (0, \cdots, 0)$ und $x = 0$, und auf $\infty$ sonst. 
Der folgende Satz aus \parencite{ff13} zeigt, dass die oben genannten rekursiven Definitionen von $C_v(\cdot, \cdot)$ verwenden werden können, um mit dynamischer Programmierung die Lösungsmenge $\bbS$ in polynomieller Zeit für konstantes $\eps$ zu berechnen. \\

\begin{thm}\label{thm:cutphase}
    Für einen beliebigen Baum $T$ und eine konstante $\eps > 0$ existiert ein Algorithmus, der $\bbS$ in polynomieller Zeit findet.
\end{thm}
\begin{proof}
    Falls $T$ nur aus einem Knoten besteht, ist der Satz offensichtlich wahr.
    Andernfalls muss in einer Optimallösung $\calS \in \bbS$ mit Signatur $\vec{g}$ Zusammenhangskomponente der Größe $x$ sein, die die Wurzel $r$ enthält.
    Diese Zusammenhangskomponente kann Größe höchstens $\mu$ haben und muss Größe mindestens $1$ haben.
    Sei nun $u$ das rechteste Kind der Wurzel $r$, dann können die Schnittkosten der $C(\vec{g})$ der Optimallösung $\calS$ in lineare Zeit mit der Formel
    \begin{equation} \label{eq:root}
        C(\vec{g}) = \min \left\{ C_u(\vec{g} - \vec{e}(x), n - x) \mid 1 \leq x < \mu \right\}
    \end{equation}
    berechnet werden.
    Die Menge von Zusammenhangskomponenten $\calS$ mit der Signatur $\vec{g}$ kann wiederum rekursiv mit Hilfe von dynamischer Programmierung und den rekursiven Formeln für $C_v$ berechnet werden.
    Die Anzahl der Knoten in den Teilbäumen, die in den linken Geschwisterknoten von $v$ und $v$ selbst gewurzelt sind, ist durch $\card{L_v}$ gegeben.
    Diese $\card{L_v}$ Knoten sind dann auf Zusammenhangskomponenten von verschidenenen Größen verteilt und stellen eine untere Grenze $\mathcal{F}$ von $L_v$ dar.
    Die Komponente $g_i$ der Signatur $\vec{g}$ zu $\mathcal{F}$ zählt Komponenten mit Größe mindestens der unteren Schranke des $i$-ten Intervalls, wie in Definition~\ref{defn:signature} spezifiziert.
    Deshalb kann man $g_i$ mit $\card{L_v}/({(1 + \eps)}^{i - 1} \cdot \eps n/k) \leq k/({(1 + \eps)}^{i - 1} \cdot \eps)$ für $i \in \{1, \ldots, t\}$ beschränken und mit $\card{L_v}$ für den Fall $i = 0$.
    Insgesamt kann man die Anzahl an Signaturen $\vec{g}$ an einem Knoten $v$ mit
    \begin{equation*}
        \card{L_v} \cdot \prod^t_{i=1} \frac{k}{{(1 + \eps)}^{i - 1} \cdot \eps} 
        = \card{L_v} \cdot {\left( \frac{k}{\eps} \right)}^t \cdot {\left( \frac{1}{1 + \eps} \right)}^{\frac{(t-1)t}{2}} 
        \leq \card{L_v} \cdot {\left( \frac{k}{\sqrt{\eps}} \right)}^t
    \end{equation*}
    nach oben beschränken. 
    Die Ungleichung gilt, da $t - 1 = \ceil*{\log_{1 + \eps}(1/\eps)}$. 
    Ferner kann $t - 1$ für $\eps \leq 1$ mit $\ceil*{1/\eps \cdot \log(1/\eps)}$ nach oben abgeschätzt werden, weil dann $1 + \eps \geq 2^\eps$ gilt.
    Schlussendlich steht fest, dass die Anzahl der Signaturen an einem Knoten $v$ gleich $\gamma \card{L_v}$ ist, wobei $\gamma \in \bigO({(k/\sqrt{\eps})}^{1 + \ceil{1/\eps \cdot \log(1/\eps)}})$.

    Um die Laufzeit zu ermitteln, wird für jeden Knoten $v \neq r$ die benötigte Anzahl der Schritte $T_v$ betrachtet, um alle Einträge $C_{v^\prime}(\vec{g}, m)$ für alle $v^\prime \in L_v$ zu bestimmen. 
    Wir stellen die Behautpung auf, dass $T_v \leq \frac{3}{2} \gamma^2 \card{L_v}^4$ für jeden Knoten $v$.
    Zur Berechnung der Signaturen am Knoten $v$ werden die Signaturen von $u$ und $w$ benötigt, welche in Zeit $T_u$ beziehungsweise $T_w$ berechnet werden können.
    Aus den Gleichungen~\eqref{eq:e_not_cut} und~\eqref{eq:e_cut} wird erkenntlich, dass wir für die Berechnung der optimalen Kosten $C_v(\vec{g}, m)$ für alle $m$ und $\vec{g}$ alle Kombinationen von $x$, $\vec{g}_u$ und $\vec{g}_w$ betrachtet werden müssen.
    Für ein festes $x$ gibt es höchstens $\gamma \card{L_u} \cdot \gamma \card{L_w}$ Möglichkeiten um die Signaturen $\vec{g}_u$ und $\vec{g}_w$ zur Signatur $\vec{g}$ zu kombinieren.
    Da $m$ und $x$ mit $\card{L_v}$ nach oben abgeschätzt werden können und $\card{L_u} + \card{L_w} \leq \card{L_v}$ gilt, erhalten wir
    \begin{equation*}
        \begin{aligned}
            T_v & \leq T_u + T_w + 2\gamma^2 \card{L_u} \card{L_w} \card{L_v}^2 \\
                & \leq \frac{3}{2} \gamma^2 \card{L_u}^4 + \frac{3}{2} \gamma^2 \card{L_w}^4 + 2\gamma^2 \card{L_u} \card{L_w} \card{L_v}^2 \\
                & \leq \frac{3}{2} \gamma^2 \card{L_v}^2 \left(\card{L_u}^2 + \card{L_w}^2 + 2 \card{L_u} \card{L_w} \right) \\
                & \leq \frac{3}{2} \gamma^2 \card{L_v}^4.
        \end{aligned}
    \end{equation*}
    Aus der Gleichung~\eqref{eq:root} folgt, dass die Berechnung jeder Signatur an der Wurzel $r$ Zeit $\bigO(\gamma n)$ benötigt.
    Deshalb ist die Laufzeit für die Schnittphase insgesamt $\bigO(\gamma^2 n^4) = \bigO(n^4{(k/\sqrt{\eps})}^{2+2 \ceil{1/\eps \cdot \log(1/\eps)}})$ und damit polynomiell für konstantes $\eps$.
\end{proof}

\subsection{Die Packphase}
In der ersten Phase gibt der Algorithmus Mengen von Zusammenhangskomponenten $\calS \in \bbS$ und deren zugehörige optimale Schnittkosten aus.
Um zu überprüfen, ob eine Menge von Zusammenhangskomponenten $\calS$ für eine gültige $(k,1+\eps)$-Partitionierung des Graphen verwendet werden kann, versucht der Algorithmus in der zweiten Phase die Komponenten in $\calS$ in höchstens $k$ Behälter der Größe $(1 + \eps) \ceil*{n/k}$ zu packen.
Dieses Problem, genannt Bin-Packing-Problem, ist generell NP-vollständig, da es eine Verallgemeinerung des stark NP-vollständigen 3-Partitionierungsproblems (siehe Satz~\ref{thm:np_comp}) ist.
Jedoch haben Hochbaum und Shmoys in \parencite{hs86} einen Algorithmus entwickelt, der das Problem in polynomieller Zeit für konstantes $\eps$ löst.
Im Weiteren wird dieser Algorithmus beschrieben, wie er in \parencite{hs86, va13} vorgestellt wurde.

Es wird gezeigt, wie eine Menge von Zusammenhangskomponenten $\calS \in \bbS$ mit der Signatur $\vec{g} = (g_0, \ldots, g_t)$ in Behälter der Größe $(1 + \eps) \ceil{n/k}$ gepackt werden kann.
Im ersten Schritt werden alle Komponenten mit Größe kleiner als $\eps \ceil*{n/k}$ entfernt, was genau die Komponenten sind, die durch den Eintrag $g_0$ gezählt werden.
Die übrigen Komponenten in $\calS$ werden anhand ihrer Größe nach Definition~\ref{defn:signature} in Intervalle gruppiert und die Größe jeder Komponente wird auf die untere Intervallgrenze des Intervalls gerundet, in dem sich die Komponente befindet.
Dadurch gibt es dann genau $g_i$ Komponenten der Größe ${(1 + \eps)}^{i-1} \cdot \eps \ceil*{n/k}$ und es wird der Vektor $(g_1, \ldots, g_t)$ verwendet, um die Bin-Packing-Instanz mit gerundeten Komponentengrößen zu beschreiben. 
Außerdem reduziert dieser Rundungschritt die Anzahl der verschieden Größen auf $t = \ceil*{\log_{1+\eps}(1/\eps)} + 1$, was ein Konstante für konstantes $\eps$ ist.
Im zweiten Schritt werden die größeren Komponenten in $\calS$ folgendermaßen in Behälter der Größe $\ceil{n/k}$ gepackt.
Für eine Bin-Packing-Instanz $\vec{i} = (i_1, \ldots, i_t)$ definiere $Bins(\vec{i})$ als die minimale Anzahl an Behältern, die für das Packen der Komponenten in $\vec{i}$ benötigt werden.
Weiterhin wird für die gegebene Instanz $(g_1, \ldots, g_t)$ die Menge $\mathcal{C} = \left\{ \vec{c} \mid Bins(\vec{c}) = 1 \right\}$ definiert, also die Menge aller Möglichkeiten einen einzigen Behälter zu füllen.
Bei $Bins(\cdot)$ handelt es sich um eine $t$-dimensionale Tabelle und der Eintrag $Bins(\vec{i})$ für $\vec{i} = (i_1, \ldots, i_t) \in \{0, \ldots, g_1\} \times \cdots \times \{0, \ldots, g_t\}$ wird berechnet mit der Rekurrenzrelation 
\begin{equation*}
    Bins((i_1, \ldots, i_t)) = 
    \begin{cases*}
        1 & für $(i_1, \ldots, i_t) \in \mathcal{C}$, \\
        1 + \min_{\vec{c} \in \mathcal{C}} Bins((i_1 - c_1, \ldots, i_t - c_t)) & sonst. \\
    \end{cases*}
\end{equation*}
Jede der Komponenten in $(g_1, \ldots, g_t)$ hat eine Größe von mindestens $\eps \ceil*{n/k}$. 
Da die Summe der Komponentengrößen höchstens die Anzahl der Knoten $n$ ergibt, kann die Anzahl der Komponenten in $(g_1, \ldots, g_t)$ mit $n / (\eps \ceil{n/k}) \leq n / (\eps \cdot n/k) = k/\eps$ nach oben abgeschätzt werden.
Daraus folgt, dass die Anzahl der Möglichkeiten einen Vektor $\vec{i} \in (i_1, \ldots, i_t) \in \{0, \ldots, g_1\} \times \cdots \times \{0, \ldots, g_t\}$ zu bilden höchstens ${(k/\eps)}^t$ ist.
Deshalb hat die $t$-dimensionale Tabelle $Bins(\cdot)$ höchstens ${(k/\eps)}^t$ Einträge. 
Weiterhin kann jeder Eintrag in Zeit ${(k/\eps)}^t$ berechnet werden, da $\mathcal{C}$ höchstens ${(k/\eps)}^t$ Vektoren enthält.
Insgesamt ist die Laufzeit dieses Schritts also $\bigO({(k/\eps)}^{2t})$.
Im dritten Schritt werden die Kapazitäten der Behälter auf $(1 + \eps) \ceil*{n/k}$ erhöht und die ursprünglichen Größen der Komponenten in $\calS$ wiederhergestellt.
Dies kann die Behälterkapazitäten nicht verletzen, da durch das Abrunden nach Definition~\ref{defn:signature} die Größe jeder Komponente um maximal \todo{Mathematisch formulieren?} einen Faktor $(1 + \eps)$ unterschätzt wurde.
Im letzten Schritt werden dann die übrigen $g_0$ Komponenten in $\calS$, deren Größe kleiner als $\eps \ceil*{n/k}$ ist, greedy mit First-Fit gepackt. 
Es wird also jede Komponente in den erstmöglichen Behälter gepackt, sodass die Behälterkapazität $(1 + \eps) \ceil*{n/k}$ nicht überschritten wird.
Wenn in keinem der Behälter mehr Platz ist, dann wird ein neuer Behälter hinzugefügt und die Komponente in diesen gepackt.
Dieser Schritt kann in Zeit \todo{$\bigO(n)$ möglich?} $\bigO(n^2)$ durchgeführt werden, da $g_0 \leq n$ gilt und es höchstens $n$ Behälter geben kann.
Zusammenfassend lässt sich sagen, dass für eine gegeben Menge von Zusammenhangskomponenten $\calS$ mit Signatur $\vec{g}$ in Zeit $\bigO({(k/\eps)}^{2t} + n^2)$ geprüft werden kann, ob $\calS$ für eine zulässige $(k, 1 + \eps)$-Partitionierung verwendet werden kann.

Vor der Zusammenführung der Schnittphase und der Packphase wird zunächst noch eine Beziehung zwischen den von der Schnittphase gegebenen Mengen von Zusammenhangskomponenten $\calS \in \bbS$ und der Menge von Zusammenhangskomponenten $\calS^*$ der optimalen perfekt ausgewogenen Partitionierung hergestellt.
Sei $\varphi(\calS)$ die Anzahl der Behälter, die benötigt werden um $\calS$ zu packen.
Für zwei Mengen von Zusammenhangskomponenten mit derselben Signatur gilt, dass die Komponenten mit mehr als $\eps \ceil*{n/k}$ Knoten immer gleich auf die Behälter verteilt werden.
Jedoch können im Greedy-Schritt die übrigen kleinen Komponenten so verteilt werden, dass für eine der beiden Mengen mehr Behälter benötigt werden.
Das folgende Lemma aus~\parencite{ff13} zeigt, dass der oben gegebene Algorithmus für eine Menge von Zusammenhangskomponenten mit derselben Signatur $\vec{g}^*$ wie die Optimallösung $\calS^*$ des $(k, 1)$-Partitionierungsproblems höchstens $k$ Behälter mit Kapazität $(1 + \eps) \ceil*{n/k}$ verwendet. \\

\begin{lem}\label{lem:bincnt}
    Sei $\calS^*$ die optimale Menge von Zusammenhangskomponenten für das $(k, 1)$-Partitionierungsproblem und sei $\vec{g}^*$ die Signatur von $\calS^*$. 
    Für eine Menge $\calS \in \bbS$ mit Signature $\vec{g}^*$ erhält man $\varphi(\calS) \leq k$.
\end{lem}
\begin{proof}
    Es können zwei Fälle auftreten: Entweder wird im Greedy-Schritt ein neuer Behälter geöffnet oder es wird kein neuer Behälter geöffnet.
    Zuerst wird der Fall betrachtet, dass das Verteilen der Komponenten mit Größe kleiner als $\eps \ceil*{n/k}$ mit First-Fit keinen neuen Behälter erzeugt.
    Dann hängt $\varphi(\calS)$ allein vom Ergebnis des Packings mit den abgerundeten Komponentengrößen ab.
    Da für ein Packing von $\calS^*$ per Definition genau $k$ Behälter benötigt werden, können die abgerundeten Komponentengrößen in maximal $k$ Behälter gepackt werden, weil der Algorithmus ein perfektes Packing für diese abgerundeten Komponentengrößen berechnet.
    Zusätzlich sind die Signaturen von $\calS$ und $\calS^*$ gleich und damit folgt, dass für ein Packing von $\calS$ maximal $k$ Behälter benötigt werden.

    Wenn der Greedy-Schritt neue Behälter erzeugt, dann ist der Füllstand der ersten $\varphi(\calS) - 1$ Behälter größer als $\ceil{n/k}$, da sonst noch Platz für Komponenten mit Größe höchstens $\eps \ceil*{n/k}$ gewesen wäre und somit der Behälter erst gar nicht erstellt worden wäre.
    Daraus folgt, dass die Anzahl der Knoten in $\calS$ strikt größer als $(\varphi(\calS))\ceil*{n/k}$ ist.
    Des Weiteren sind die Anzahl der Knoten in $\calS$ und $\calS^*$ gleich und es folgt, dass mindestens $\varphi(\calS)$ Behälter benötigt werden um $\calS^*$ in Behälter der Größe $\ceil*{n/k}$ zu packen.
\end{proof}

    
    Die Schnittphase und die Packphase wird zusammengeführt, indem wir als Ausgabe des Algorithmus die Menge $\calS \in \bbS$ mit minimalen Kosten unter den Mengen wählen, für die $\varphi(\calS) \leq k$ gilt.
    Im nächsten Theorem, welches aus~\parencite{ff13} stammt, wird die Korrektheit und die Laufzeit des Algorithmus gezeigt. \\

    \begin{thm}\label{thm:treealg}
        Für einen Baum mit positiven Kantengewichten und Parameter $\eps > 0$ und $k \in \{1, \ldots, n\}$ existiert ein Algorithmus, der eine Partitionierung der Knoten von $T$ in $k$ Mengen berechnet, sodass jede der $k$ Mengen höchstens eine Größe von $(1 + \eps) \ceil*{n/k}$ hat und die Schnittkosten höchstens die Schnittkosten einer optimalen perfekt ausgewogenen Partitionierung des Baums entsprechen. 
        Weiterhin ist die Laufzeit für konstantes $\eps$ polynomiell.
    \end{thm}
    \begin{proof}
        Sei $\tildaS \in \bbS$ die Menge von Zusammenhangskomponenten, die durch den Algorithmus zurückgegeben wird, und seien $C(\vec{g})$ die Schnittkosten einer Menge $\calS \in \bbS$ mit Signatur $\vec{g}$, dann
        \begin{equation}\label{eq:laststep}
            \tildaS = \arg_{\calS \in \bbS} \min \left\{ C(\vec{g}) \mid \calS\ \text{hat Signatur}\ \vec{g} \land \varphi(\calS) \leq k \right\}.
        \end{equation}

    Lemma~\ref{lem:bincnt} zeigt für eine Menge von Zusammenhangskomponenten $\calS \in \bbS$ mit Signatur $\vec{g}^*$, dass $\varphi(\calS) \leq k$ gilt.
    Aus der Minimierung in \eqref{eq:laststep} folgt, dass die Schnittkosten von $\tildaS$ höchstens die der Menge von Zusammenhangskomponenten $\calS \in \bbS$ mit Signature $\vec{g}^*$ ist.
    Da $\calS \in \bbS$ mit Signatur $\vec{g}$ jeweils die Menge mit minimalen Schnittkosten unter denen mit der Signatur $\vec{g}$ ist, folgt, dass die Schnittkosten von $\tildaS$ höchstens die von $\calS^*$ sind, was den Korrektheitsbeweis erbringt.

        Nun wird die Laufzeit betrachtet und dazu wird ein Ergebnis aus dem Beweis von Satz~\ref{thm:cutphase} verwendet, welches besagt, dass die Anzahl der Signaturen die in der Schnittphase betrachtet werden $\gamma n$ ist, wobei $\gamma \in \bigO({(k/\sqrt{\eps})}^{1 + \ceil{1/\eps \cdot \log(1/\eps)}})$. 
        Des Weiteren zeigt Satz~\ref{thm:cutphase}, dass $\bbS$ höchstens $\gamma n$ Komponenten enthält und in Zeit $\bigO(n^4 \gamma^2)$ berechnet werden kann. 
        Für jede dieser Mengen $\calS \in \bbS$ kann in der Packphase in Zeit $\bigO({(k/\eps)}^{2t} + n^2)$ bestimmt werden, ob es möglich ist $\calS$ in höchstens $k$ Behälter zu packen.
        Deshalb kann die Packphase in Zeit $\bigO(\gamma n ({(k/\eps)}^{2t} + n^2))$ durchgeführt werden.
        \todo{Laufzeit insgesamt} Laufzeit insgesamt?
    \end{proof}
    
\subsection{Erweiterung auf allgemeine Graphen}
Aufgrund von Theorem~\ref{thm:treealg} existiert ein Algorithmus, der das $(k,1+\eps)$-Partitionierungsproblem für konstantes $\eps$ auf Bäumen löst und damit ein PTAS für das $(k,1)$-Partitionierungsproblem ist.
In diesem Abschnitt wird der Algorithmus auf generelle Graphen erweitert.
Dabei hat der präsentierte Algorithmus für generelle Graphen $G$ mit Kantengewichten einen Approximationsfaktor von $\bigO(\log \, n)$, das heißt, die vom Algorithmus gefundene Partitionierung hat Schnittkosten, die höchstens um einen Faktor in $\bigO(\log \, n)$ von den optimalen Schnittkosten entfernt sind.
Für diesen Algorithmus wird ein von Räcke entwickeltes Verfahren~\parencite{rc08} verwendet, das für einen Graphen $G$ eine Menge von Dekompositionsbäumen finden, deren Schnittkosten die des ursprünglichen Graphen gut approximieren. 
Dann wird der Algorithmus der in den vorherigen Sektionen präsentiert wurde, um fast ausgewogene Partitionierungen auf diesen Dekompositionsbäumen zu finden.
Im Folgenden werden nur die nötigen grundlegenden Konzepte von Dekompositionsbäumen eingeführt, für ein ausführlichere Diskussion von Dekompositionsbäumen siehe \parencite{rc08, ma10, ws11}.

Sei $G = (V, E, \weight)$ ein Graph mit Gewichtsfunktion $\weight : E \rightarrow \reals^+$, dann sind die Schnittkosten $\weight(W)$ eines Schnittes $W \subseteq V$ durch die Summe der Gewichte der Kanten gegeben, die Knoten in $W$ mit Knoten in $V \setminus W$ verbinden.
