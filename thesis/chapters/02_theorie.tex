% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\newcommand{\apxalg}{\mathcal{A}}
\newcommand{\weights}{\omega}

\chapter{Theoretischer Hintergrund}\label{chapter:theorie}
\section{Berechnungskomplexität von perfekt ausgewogenen Partitionierungen \todo{Titel!}}
Wie bereits im vorherigem Kapitel geschildert, ist das $(2,1)$-Partitionierungsproblem, auch \todo{Minimum-Bisection erklären?} Minimum-Bisection-Problem genannt, ein wohlbekanntes Thema in der Forschung. 
Es ist bereits länger bekannt, dass es sich beim Minimum-Bisection-Problem um ein NP-vollständiges Problem handelt~\parencite{gj79}.
In jüngerer Vergangenheit wurde von Feige und Krauthgamer ein polynomieller Algorithmus präsentiert, der das Problem mit einem polylogarithmischen Approximationsfaktor löst. \parencite{fk02}
Wie sich herausstellte, lässt sich dieses Ergebnis jedoch nicht auf auf das $(k,1)$-Partitionierungsproblem übertragen.
Es kann nämlich keinen polynomiellen Approximationsalgorithmus für das $(k,1)$-Partitionierungsproblem, wobei $k$ keine Konstante ist, geben.
Diese Behautpung wird im folgenden Satz, der aus \parencite{ar06} entnommen ist, gezeigt. \\

\begin{thm}
    Vorausgesetzt $P \neq NP$, dann gibt es für das $(k,1)$-Partitionierungsproblem keinen polynomiellen Approximationsalgorithmus mit endlichem Approximationsfaktor.
\end{thm}
\begin{proof}
    Um die Aussage zu zeigen, reduzieren wir das $3$-Partitionierungsproblem auf das $(k,1)$-Partitionierungsproblem. 
    Das $3$-Partitionierungsproblem ist folgendermaßen definiert: Gegeben $n = 3k$ ganze Zahlen $a_1,\ldots, a_n$ und einen Schwellwert $A$, sodass $\frac{A}{4} < a_i < \frac{A}{2}$ für alle $i \in [n]$ gilt, und 
    \begin{equation*}
        \sum_{i=1}^{n} a_i = kA.
    \end{equation*}
    Nun soll entschieden werden, ob eine Partitionierung der Zahlen $a_1, \ldots, a_n$ in $k$ Tripel, die alle zu $A$ summieren, möglich ist. 
    In \parencite{gj79} wurde die starke NP-Vollständigkeit dieses Problems gezeigt, das heißt es ist auch schon NP-vollständig, wenn sowohl alle $a_i$, als auch $A$, polynomiell beschränkt sind.
    Seien die $a_i$ und $A$ im Folgenden polynomiell beschränkt.

    \noindent Wir nehmen zum Widerspruch an, dass wir einen Approximationsalgorithmus $\apxalg$ mit endlichem Approximationsfaktor für das $(k,1)$-Paritionierungsproblem haben. 
    Wir zeigen nun, dass wir $\apxalg$ benutzen können, um eine Instanz des $3$-Partitionierungsproblems zu lösen.
    Dazu konstruieren wir einen Graphen $G$, in dem es für jede der gegebenen Zahlen $a_i$ ein Clique mit $a_i$ Knoten gibt.
	Man bemerke dabei, dass $G$ in polynomieller Zeit konstruiert werden kann, da alle $a_i$ und $A$ polynomiell beschränkt sind.
    Wenn es eine Lösung für die Instanz des $3$-Partitionierungsproblems gibt, dann findet $\apxalg$ eine Lösung die keine Kanten in $G$ schneidet. 
    Auf der anderen Seite schneidet $\apxalg$ mindestens eine Kante in $G$, wenn keine Lösung existiert.
    Mit $\apxalg$ kann also entschieden werden, ob eine Lösung existiert, und somit kann das $3$-Partitionierungsproblem gelöst werden.
    Dies ist jedoch ein Widerspruch zur Annahme, dass es unter der Voraussetzung $P \neq NP$ keinen Algorithmus gibt, der das $3$-Partitionierungsproblem in polynomieller Zeit löst.
\end{proof}

Weiterhin wurde dieses Ergebnis in \parencite{ff13} verfeinert. 
Dort wurde gezeigt, dass das $(k,1)$-Partitionierungsproblem auch dann NP-vollständig bleibt, wenn man sich auf Bäume als Eingabegraphen beschränkt.
Insbesondere wurden ungewichtete Bäume mit Maximaldurchmesser $4$, das heißt die Länge des längsten Pfades zwischen zwei beliebigen Blättern ist höchstens $4$, betrachtet.
Schon für diese Bäume kann für das $(k,1)$-Partitionierungsproblem kein Approximationsalgorithmus mit Approximationsfaktor $n^c$ für beliebige Konstanten $c < 1$ gefunden werden.
Auch für ungewichtete Bäume mit maximalen Knotengrad $5$ kann es keinen polynomiellen Algorithmus geben.

Die vorangegangenen Erkenntisse zeigen, dass auch für bestimmte eingeschränkte Graphen kein Approximationsalgorithmus für das $(k,1)$-Partitionierungsproblem gefunden werden kann, außer es gilt $P=NP$. 
Deshalb liegt die Verwendung eines bikriteriellen Algorithmus nahe, also ein Algorithmus, der die Anforderung relaxiert, dass alle $k$ Teile die gleiche Größe haben.
Dabei wird, anstatt das $(k,1)$-Partitionierungsproblem zu lösen, stattdessen das $(k,1+\eps)$-Partitionierungsproblem mit $\eps > 0$ betrachtet.
Für dieses Problem wurde von Andreev und Räcke ein polynomieller Algorithmus mit Approximationsfaktor $\bigO(log^{1.5}\, n / \eps^2)$ für konstantes $\eps > 0$ präsentiert. \parencite{ar06} 

\section{Ein bikriterieller Graphpartitionerungsalgorithmus}
Wie im vorherigem Abschnitt gezeigt, ist das $(k,1)$-Partitionierungsproblems schwer zu approximieren. 
Deshalb wurde ein bikriterieller Ansatz nahegelegt, das bedeutet man approximiert in Hinsicht auf die Ausgeglichenheit der Partitionenierung und in Hinsicht auf die Schnittkosten.
In diesem Abschnitt wird ein polynomieller bikriterieller Approximationsalgorithmus mit Approximationsfaktor $\bigO(log\, n)$, wie in \parencite{ff13} beschrieben, vorgestellt.
Dieser Algorithmus arbeitet ausschließlich auf Bäumen, er kann jedoch auf generelle Graphen erweitert werden, indem man den Eingabegraphen vorher in einen Baum mit ähnlichen Schnittkosten umwandelt.
Für diese Umwandlung wird ein Verfahren verwendet, das von Räcke entwickelt wurde. \parencite{rc08}

\subsection{Fast ausgewogene Partitionierung von Bäumen}
In dieser Sektion wird das $(k,1+\eps)$-Partitionierungsproblem auf Bäumen mit Kantengewichten untersucht. 
Die folgenden Ergebnisse werden zeigen, dass es für Bäume mit Kantengewichten einen polynomiellen Algorithmus gibt, der das Problem löst und ferner eine Lösung nicht schlechter als die Optimallösung des $(k,1)$-Partitionierungsproblems im Bezug auf die Schnittkosten liefert.
Damit ist dieser Algorithmus ein \todo{PTAS einführen} PTAS für das $(k,1)$-Partitionierungsproblem auf Bäumen. 
Gegeben sei ein Baum $T = (V,E,\weights)$ mit $n$ Knoten, wobei $\weights : E \rightarrow \reals^+$ die Gewichtsfunktion ist.
Prinzipiell löst das folgende Verfahren das $(k,1)$-Partitionierungsproblem in zwei Schritten.
Im ersten Schritt werden alle Möglichkeiten berechnet den Baum $T$ in Zusammenhangskomponenten zu zerlegen.
Diese Zusammenhangskomponenten werden der Größe nach gruppiert.
Dann kann eine Partitionierung von $T$ durch eine Menge von disjunkten Zusammenhangskomponenten dargestellt werden, die zusammen $T$ ergeben. 
Diese Mengen von Zusammenhangskomponenten werden nun anhand der Größen der enthaltenen Komponenten in Äquivalenzklassen eingeordnet, wobei zwei Mengen genau dann äquivalent sind, wenn sie dieselbe Anzahl an Komponenten der Größe $x$ für alle $x \in \{1, \ldots, \ceil*{n/k}\}$ enthalten.
Für jede Äquivalenzklasse $\mathcal{S}$ wird die Partitionierung mit den geringsten Schnittkosten als Repräsentant für $\mathcal{S}$ gewählt.
Im zweiten Schritt werden dann nur die Äquivalenzklassen betrachtet, deren enthaltene Zusammenhangskomponenten sich in $k$ Mengen mit Größe höchstens $\ceil*{n/k}$ packen lassen und somit eine gültige Partitionierung darstellen.
Anschließend wird als Lösung der Repräsentant der übrigen Äquivalenzklasse ausgewählt, der die niedrigsten Schnittkosten verursacht.
Offensichtlich kann mit diesem Verfahren eine optimale Lösung für das $(k,1)$-Partitionierungsproblem gefunden werden.
Die Laufzeit dieses Verfahrens ist jedoch exponentiell in der Anzahl der Knoten $n$, da insbesondere die Anzahl der Äquivalenzklassen exponentiell in $n$ ist.

Um eine polynomieller Laufzeit zu erreichen, werden die Zusammenhangskomponenten in gröbere Äquivalenzklassen eingeordnet.
Anstatt Komponenten nach der genauen Größe zu gruppieren, werden die möglichen Größen in Intervalle aufgeteilt, das heißt Komponenten mit verschiedenen Größen werden entsprechend in Intervallen zusammengefasst. 
Wie oben gehören zwei Mengen zur selben Äquivalenzklasse, wenn sie für jedes der Intervalle gleich viele Komponenten in diesem Größenintervall haben.
Durch eine geschickte Wahl der Intervalle in Abhängigkeit von $\eps$ kann die Anzahl der Äquivalenzklassen für konstantes $\eps$ auf eine polynomielle Anzahl im Bezug auf $n$ beschränkt werden.
Diese groben Äquivalenzklassen verursachen jedoch einen Approximationsfehler und man erhält keine perfekt ausgewogene Partitionierung mehr sondern eine $(k, 1+\eps)$-Partitionierung. Die folgende Definition legt die Wahl der Intervalle fest. \\

\begin{defn}[Signatur]
    Sei $\mathcal{S}$ eine Menge von Zusammenhangskomponenten über der Knotenmenge $V$ von $T$ und $\eps > 0$. Ein Vektor $\vec{g} = (g_0, \ldots, g_t)$ mit $t = \ceil*{log_{1+\eps}(1/e)} + 1$ wird Signatur von $\mathcal{S}$ genannt, wenn $\mathcal{S}$ genau $g_0$ Komponenten mit Größe in $[1,\, \eps \ceil*{n/k})$ und $g_i$ Komponenten mit Größe in $[(1+\eps)^{i-1}\, \cdot\, \eps \ceil*{n/k},\, (1+\eps)^i\, \cdot \, \eps \ceil*{n/k})$ für $i \in \{1, \ldots, t\}$ enthält.
\end{defn}

In einer groben Äquivalenzklasse befinden sich alle Mengen von Zusammenhangskomponenten, die dieselbe Signature haben.
Der Repräsentant ist also die Menge von Zusammenhangskomponenten, die unter allen Mengen mit der gleichen Signatur die geringsten Schnittkosten aufweist.
Im ersten Schritt verwendet man dynamische Programmierung, um die Signaturen und die zugehörigen Repräsentanten zu finden.
Dabei verwendet einen Ansatz, der eine Verallgemeinerung des Ansatzes für das Bisektionsproblem(vgl. \parencite{mcg78, ws11}) ist.
Die dynamische Programmierung findet für jede Signatur einen Repräsentanten optimalen Schnittkosten, der alle Knoten enthält.
Sei nun $\mathbb{S}$ die Menge dieser Repräsentanten.
Im zweiten Schritt versucht der Algorithmus für jeden Repräsentanten $\mathcal{S} \in \mathbb{S}$ die enthaltenen Zusammenhangskomponenten in Behälter mit Kapazität $(1+\eps)\ceil*{n/k}$ zu packen. 
Hierbei handelt es sich um das Bin Packing Problem, wofür von Hochbaum und Shmoys in \parencite{hs86} ein Approximationsalgorithmus vorgestellt wurde, der in \parencite{va13} weiterentwickelt wurde.
Die schlussendliche Ausgabe des Algorithmus ist eine Partitionierung des Graphen gegeben durch das vom Bin Packing Algorithmus gefundene Packing.
Für die Ausgabe wird unter den Repräsentanten, deren Komponenten sich in höchstens $k$ Behälter packen lassen, der Repräsentant mit den geringsten Schnittkosten ausgewählt.
Beide Schritte des Algorithmus haben für konstantes $\eps$ eine polynomielle Laufzeit.

\subsection{Die Schnittphase}
\subsection{Die Packphase}
\subsection{Erweiterung auf allgemeine Graphen}


