% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Diskussion}\label{chapter:diskussion}
\section{Laufzeit}
% Verdopplung der Knotenanzahl -> ca. 10-fache Laufzeit -> $n^4$
% Exponentielle Laufzeitsteigerung sichtbar, Ansprechen der höheren Streuung bei niedrigem $\varepsilon$
% Teilanzahl, warum hoch und runter, Mehr Teile -> Feinere Schritte -> mehr Signaturen, Mehr Teile -> weniger Knoten pro Signature
% Binärbaum hohe Laufzeit, Fat Tree niedrige Laufzeit, Grad hoch -> Laufzeit runter, Verweis auf Schwierigkeitk
In Satz~\ref{thm:cutphase} wurde bewiesen, dass die Schnittphase des Algorithmus in Zeit $\bigO(\gamma^2 n^4)$ durchgeführt werden kann, wobei $\gamma \in \bigO\big((k/\sqrt{\eps})^{1+\ceil{1/\eps \cdot \log(1/\eps)}}\big)$.
Außerdem wurde in Sektion~\ref{sec:packing} gezeigt, dass die Packphase eine Laufzeit von $\bigO\big(\gamma n ((k/\eps)^{2t} + n^2)\big)$ hat, wobei $t = \ceil{\log_{1 + \eps}(1/\eps)} + 1$ gilt.
Damit erhalten wir für konstantes $k$ und $\eps$ eine quartisch ansteigende Laufzeit proportional zu $n$.
Die Werte in Tabelle~\ref{tab:highnodecnt} bestätigen die Analyse, da eine Verdopplung der Knotenanzahl die Laufzeit um mehr als das $16$\hyp fachen erhöht.
Auch in Abbildung~\ref{fig:runnodes} wird ersichtlich, dass eine Erhöhung der Knotenanzahl von $70$ auf $130$ eine ähnliche Auswirkung auf die Laufzeit hat.
Da die Laufzeit für größere Werte von $k$ und kleinere von $\eps$ auch ansteigt, kann der Algorithmus in der Praxis nicht für größere Bäume eingesetzt werden.
Dies steht im Gegensatz zu den Heuristiken METIS und KaHIP, welche in der Lage sind Graphen mit mehr als $100000$ Knoten zu partitionieren.~\cite{KK98, SS13}

Eine weitere Beobachtung, die aus Abbildung~\ref{fig:runnodes} hervorgeht, ist die geringe Streuung der Messwerte bei vollständig balancierten Binärbäumen, die durch das in Sektion~\ref{sec:exprun} beschrieben Verfahren generiert wurden.
Da jedes Level des Binärbaums bis auf das letzte vollständig mit Knoten gefüllt ist, haben alle generierten Bäume für eine feste Knotenanzahl die gleiche Struktur.
Es gilt, dass für konstantes $k$ und $\eps$ die Schnittphase die Laufzeit dominiert.
Deshalb führen alle generierten Binärbäume zur einer ähnlichen Laufzeit, da die berechneten Signaturen in der Schnittphase allein von der Struktur des Baums abhängig sind.
Nur die Kosten der jeweiligen Signatur sind von Baum zu Baum unterschiedlich.
In Sektion~\ref{sec:exprun} wurde weiterhin angemerkt, dass Fat Trees zu einer verhältnismäßig geringen und Binärbäume zu einer verhältnismäßig hohen Laufzeit führen.
Auf dieses Verhalten wird weiter unten genauer eingegangen.

Nun wird die Auswirkung des maximalen Ungleichgewichts der Partitionierung $\eps$ auf die Laufzeit untersucht, das ausschließlich einen negativen Einfluss auf die Laufzeit hat.
Während bei der Knotenanzahl $n$ steigende Werte die Laufzeit erhöhen, sind es bei $\eps$ fallende.
Die theoretische Analyse der Laufzeiten für die Schnittphase und die Packphase aus Sektion~\ref{sec:treepartitioning} zeigt, dass die Laufzeit für konstantes $n$ und $k$ mindestens exponentiell mit fallendem $\eps$ steigt.
Die Werte für Random Attachment, Preferential Attachment und Binärbäume in Abbildung~\ref{fig:runimb} lassen für die Parameterwahl $n = 100$ und $k = 2$ zumindest eine exponentielle Entwicklung erahnen.

Der letzte Parameter des Algorithmus ist die Anzahl der Partitionen $k$. 
Aus der Sektion~\ref{sec:treepartitioning} wissen wir, dass $k$ sowohl in die Laufzeit der Schnittphase, als auch in die Laufzeit der Packphase einfließt.
Dementsprechend sollte die Laufzeit mit steigendem $k$ zunehmen.
Die Abbildung~\ref{fig:runkparts} bestätigt diese Annahme jedoch nicht für alle Parameterkombinationen.
Für $n=50$ und $\eps=1/3$ beziehungsweise $\eps=1/4$ nimmt die Laufzeit zwar zunächst zu, dann jedoch wieder ab.
Die Zunahme der Laufzeit kann mit der steigenden Anzahl der Signaturen erklärt werden.
Da die Signaturen nach Definition~\ref{defn:signature} für ein gegebenes $\eps$ immer dieselbe Länge haben, werden die Intervallabstände der Signatur kleiner für wachsendes $k$, was die Anzahl der möglichen Signaturen erhöht.
Für höheres $k$ nimmt jedoch die maximale Größe $(1 + \eps) \ceil{n/k}$ einer Zusammenhangskomponente ab und die Intervallabstände werden so klein, dass einige Intervalle keine Zusammenhangskomponenten mehr enthalten können. 
Diese Entwicklung ist in der Tabelle~\ref{tab:ksig} zu sehen, welche die jeweiligen exklusiven oberen Schranken einer Signatur $\vec{g} = (g_0, \ldots, g_t)$ für die Parameter $n = 50$ und $\eps = 1/4$ in Abhängigkeit von $k$ abbildet.
Diese Parameter wurden auch für in Abbildung~\ref{fig:runkparts} verwendet.
Auf der anderen Seite zeigt die Abbildung~\ref{fig:runkparts}, dass schon für $n=60$ der oben genannte Effekt für die gegebenen $k$ nicht mehr eintritt. 

\begin{table}
    \centering
    \begin{tabular}{l*{11}{c}}
        \toprule
        $k$ & $g_0$ & $g_1$ & $g_2$ & $g_3$ & $g_4$ & $g_5$ & $g_6$ & $g_7$ & $g_8$ \\
        \midrule
        3 & 5 & 6 & 7 & 9 & 11 & 13 & 17 & 21 & 22 \\
        4 & 4 & 5 & 6 & 7 & 8 & 10 & 13 & 16 & 17 \\
        5 & 3 & 4 & 4 & 5 & 7 & 8 & 10 & 12 & 13 \\ 
        6 & 3 & 3 & 4 & 5 & 6 & 7 & 9 & 11 & 12 \\ 
        8 & 2 & 3 & 3 & 4 & 5 & 6 & 7 & 9 & 9 \\ 
        10 & 2 & 2 & 2 & 3 & 4 & 4 & 5 & 6 & 7 \\ 
        \bottomrule
    \end{tabular}
    \caption{Exklusive obere Schranken der Signatur $\vec{g}$ für $n=50$ und $\eps=1/4$ in Abhängigkeit von $k$}\label{tab:ksig}
\end{table}

Zuletzt wird die Entwicklung der Laufzeit für steigenden Maximalgrad $\Delta$ des Baums analysiert.
Feldmann und Foschini~\cite{FF15} zeigten, dass das $(k, 1)$\hyp Partitionierungsproblem auf generellen Graphen ab einem Maximalgrad von $\Delta=5$ $NP$\hyp schwer und ab $\Delta=7$ $APX$-schwer ist.
Für einen Maximalgrad von $\Delta=2$ ist das Problem noch in $P$, da es sich bei dem Graphen dann um einen Pfad oder einen Kreis handelt.
Weiterhin kann ein Algorithmus von Macgregor~\cite{mcg78} angepasst werden, um einen Algorithmus mit Approximationsfaktor $\bigO(\Delta \log_\Delta(n/k))$ zu erhalten.~\cite{FF15}
Daraus folgt, dass das Problem mit wachsendem Maximalgrad $\Delta$ schwieriger wird.
Die Experimente, welche durch Abbildung~\ref{fig:rundeg} visualisiert werden, zeigen jedoch, dass die Laufzeit mit wachsendem Maximalgrad abnimmt.
Um eine Intuition zu geben, warum dies der Fall ist, müssen wir zuerst betrachten, wodurch die Laufzeit verringert wird.
Die Laufzeit ist dann gering, wenn wenige Signaturen berechnet werden.
An einem Knoten $v$ des Baums werden dann wenige Signaturen berechnet, wenn der Knoten keine linken Geschwisterknoten oder keine Kindknoten hat, weil nicht existierende Knoten nur die Signatur $(0, \ldots, 0)$ besitzen (siehe Sektion~\ref{sec:cutting}).
Wenn mindestens eine der Knoten nicht existiert, dann müssen die Signaturen an dem anderen Knoten nur mit der Signatur $(0, \ldots, 0)$ und allen Signaturen $\vec{e}(x)$ kombiniert werden.
Dabei ist $\vec{e}(x)$ die Signatur mit einer Komponente der Größe $x$ und $x$ läuft über alle möglichen Größen der Komponente, in der $v$ enthalten ist.
Wenn alle Knoten im Baum einen geringen Grad haben, dann ist die Anzahl der Knoten die keinen linken Geschwisterknoten hoch. 
Andererseits ist die Anzahl der Knoten ohne rechtes Kind dann hoch, wenn alle Knoten einen hohen Grad haben.
Warum also sollte die Laufzeit sinken?

Für ein intuitives Verständnis betrachten wir dazu nur den Spezialfall eines vollständigen balancierten $d$\hyp ären Baums, der einen Maximalgrad von $\Delta = d + 1$ hat.
Die Anzahl der Knoten in einem $d$\hyp ären Baum $T$ mit Höhe $h$ ist $n = 1 + d + d^2 + \cdots + d^h = (d^{h+1} - 1)/(d - 1)$.
Daraus folgt, dass $T$ eine Höhe von $h = \log_d((d-1)n+1))-1$ hat.
Außerdem wissen wir, dass die Anzahl der Blätter $d^h$ ist, was genau die Knoten ohne Kinder sind.
Die Anzahl der Blätter kann in Bezug zu $n$ gesetzt werden, indem wir berechnen, wie viele Knoten keine Blätter sind:
\begin{equation*}
    n - d^h = \frac{d^{h+1} - 1}{d - 1}  - d^h = \frac{d^h - 1}{d - 1}.
\end{equation*}

Anders ausgedrückt hat $T$ genau $n - (d^h - 1)/(d - 1)$ Kinder.
Um die Anzahl der Knoten zu bestimmen, die keinen linken Geschwisterknoten haben, betrachten wir die Level des Baums.
In jedem Level hat der erste Knoten des Levels keinen linken Geschwisterknoten.
Ferner hat auch jeder $d+1$\hyp te Knoten keinen linken Geschwisterknoten, da sonst der Elternknoten mehr als $d$ Kinder haben müsste.
Da das Level $\ell$ genau $d^{\ell}$ Knoten enthält, ist die Anzahl der Knoten ohne linken Geschwisterknoten
\begin{equation}\label{eq:woleftsib}
    \sum_{\ell=1}^{h} \frac{d^\ell}{d} = \frac{d^h - 1}{d - 1}.
\end{equation}

Wir haben dabei die Wurzel ignoriert, weil diese nie einen linken Geschwisterknoten hat.
Aus den obigen Gleichungen folgern wir, dass die Anzahl der Knoten ohne Kind beziehungsweise ohne linken Geschwisterknoten gleich $n$ ist.
Die Summe ist gleich $n$, weil wir Knoten, die weder ein Kind noch einen linken Geschwisterknoten haben, doppelt zählen.
Aus der Gleichung~\eqref{eq:woleftsib} können wir folgern, dass die Anzahl der Blätter, die keinen linken Geschwisterknoten haben, gleich $d^h / d$ ist.
Deshalb zählen wir genau $d^h / d$ Knoten doppelt.
Nun können wir die Anzahl der Knoten berechnen, die sowohl einen linken Geschwisterknoten als auch ein rechtes Kind haben und erhalten
\begin{equation*}
    n - \left(n - \frac{d^h}{d}\right) = \frac{d^{\log_d((d-1)n + 1) - 1}}{d} = \frac{dn - n + 1}{d^2} \overset{n \geq 1}{\leq} \frac{n}{d}.
\end{equation*}

Daraus folgt, dass die Anzahl der Knoten an denen die Berechnung der Signaturen teuer ist, da sie sowohl einen linken Geschwisterknoten als auch ein rechtes Kind haben, mit steigendem Grad sinkt und damit nimmt die Laufzeit insgesamt ab.


\section{Lösungsqualität}
Der Algorithmus liefert nach Satz~\ref{thm:treealg} auf einem Baum eine Lösung, die mindestens so gut ist wie die Optimallösung des $(k, 1)$\hyp Partitionierungsproblems.
Auf Bäumen ist damit die niedrige relative Lösungsqualität der Heuristiken nicht überraschend, da diese keinerlei Garantien im Bezug auf die Kosten der Partitionierung bieten.
Außerdem ist die Streuung der Messergebnisse hoch. 
Des Weiteren treten für METIS recursive und METIS k-way Ausreißer mit einer sehr niedrigen relativen Lösungsqualität auf.
KaFFPa hingegen weist insgesamt eine höhere relative Lösungsqualität auf und es treten nur wenige Ausreißer unter $0.2$ auf.
Da KaFFPa in der zehnten DIMACS Implementation Challenge~\cite{BMS+13} im Vergleich zu anderen Heuristiken die beste Lösungsqualität lieferte, erfüllen diese Ergebnisse die Erwartungen.

Auf der anderen Seite gelang es den Heuristiken in manchen Fällen eine bessere Lösung zu finden als der Algorithmus dieser Arbeit.
Der Algorithmus bietet zwar eine Garantie im Hinblick auf die Lösungsqualität, jedoch bezieht sich die Garantie auf die Optimallösung des $(k,1)$\hyp Partitionierungsproblems.
Im Allgemeinen kann die Optimallösung des $(k, 1+\eps)$\hyp Partitionierungsproblems, welches der Algorithmus und die Heuristiken lösen, deutlich geringere Schnittkosten haben und deshalb ist es möglich, dass die Heuristiken ein besseres Ergebnis liefern.
Hinzu kommt, dass die Heuristiken sich in manchen Fällen nicht an das gegebene Ungleichgewicht halten.
Der Grund dafür sind Rundungsfehler, da der Algorithmus dieser Arbeit die Größenbeschränkungen mit Hilfe von rationalen Zahlen exakt berechnet, währenddessen die Heuristiken das Ungleichgewicht $\eps$ als Fließkommazahl darstellen.

Insgesamt bietet der Algorithmus dieser Arbeit für kleine Werte von $n$ und $k$ und für nicht zu kleines $\eps$ auf Bäumen eine Alternative zu den Heuristiken, wenn eine hohe Qualität der Lösung erwünscht ist und die höhere Laufzeit unproblematisch ist.
Dabei ist Lösungsqualität recht konsistent für verschiedene Kombinationen von Parametern und Eingabebäumen.
Allerdings ist der Algorithmus für kleinere Werte von $\eps$ aufgrund der hohen Laufzeit nicht verwendbar.
Für geringe Werte von $k$ ist ein Algorithmus von Macgregor~\cite{mcg78} eine Alternative.
Dieser Algorithmus berechnet auf Bäumen eine optimale Lösung des $(k,1)$\hyp Partitionierungsproblems in polynomieller Zeit, wenn $k$ konstant ist.

Die Zufallsgraphen werden vor der Partitionierung in Bäume umgewandelt. 
Dennoch findet der Algorithmus günstige Partitionierungen, wenngleich diese nur sehr selten besser sind als die Partitionierungen der Heuristiken.
Ausschlaggebend ist dabei, dass der Algorithmus mit Graphen des Barabási-Albert-Modells besser funktioniert, welche eher realen Graphen ähneln.~\cite{AB02}
Da die Heuristiken bei höheren Knotenanzahlen den Graph kontrahieren müssen, wäre zu erwarten, dass die relative Lösungsqualität mit wachsender Knotenanzahl sinkt.
In den Experimenten wird das nicht sichtbar, da die Knotenanzahlen zu gering sind.
Experimente mit höherer Knotenanzahl konnten aufgrund der hohen Laufzeit nicht durchgeführt werden.

Außerdem zeigen die Experimente, dass die hierarchische Dekomposition für die zufälligen Eingabegraphen meist zur besten Performanz führt.
Dieses Verhalten ist absehbar, weil die hierarchische Dekomposition im Gegensatz zu den Spannbäumen die Kosten der Schnitte im Eingabegraphen approximiert.
Deshalb wurde für die Experimente auf Graphen nur die hierarchischen Dekomposition betrachtet.
Besonders fällt auf, dass bei Verwendung der hierarchischen Dekomposition die Performanz für $k=2$ deutlich besser ist, wobei die relativen Lösungsqualität der Heuristiken mit fallendem $\eps$ zunimmt.
Damit ist der Algorithmus dieser Arbeit in Kombination mit der hierarchischen Dekomposition am besten zur bikriteriellen Approximation des Minimum-Bisection-Problems geeignet.
Des Weiteren sind die relativen Schnittkosten für höhere Kantenanzahlen geringer.
Die Heuristiken scheinen im Vergleich zur hierarchischen Dekomposition größere Probleme mit den dichteren Zufallsgraphen zu haben.

Der Datensatz as199902829 ist interessant, da es sich dabei um einen Graphen mit $103$~Knoten handelt und dieser deshalb vorher nicht kontrahiert werden muss.
Die Tatsache dass der Algorithmus im Vergleich zu METIS recursive und METIS k-way, eine gute relative Performanz liefert, zeigt, dass der Algorithmus für kleine Graphen geeignet ist.
Außerdem ist die relative Lösungsqualität etwas besser, als es die Experimente auf Zufallsgraphen hätten vermuten lassen.
Weiterhin liefert KaFFPa auch für diesen Eingabegraphen Partitionierungen mit hoher Qualität, wobei KaFFPa und der Algorithmus dieser Arbeit in drei Fällen Partitionierungen mit identischen Schnittkosten finden.

Für die Datensätze email-Eu-core und ca-GrQc mussten zunächst Kanten kontrahiert werden.
In diesem Sinne ist die relative Lösungsqualität immer noch niedrig.
Es muss dabei beachtet werden, dass die Heuristiken für diese Knotenanzahl eventuell auch Kanten kontrahieren.
Während der Unterschied zwischen METIS und KaFFPa für email-Eu-core nicht hoch ist, setzt sich KaFFPa für den Datensatz ca-GrQc, der fünfmal mehr Knoten als email-Eu-core hat, deutlich von METIS recursive und k-way ab.
Die tendenziell schlechtere Lösungsqualität für die größeren Graphen wird mitunter dadurch verstärkt, dass der Algorithmus dieser Arbeit im Gegensatz zu den Heuristiken keine lokalen Verbesserungen beim Expandieren der zuvor kontrahierten Kanten durchführt.

Obwohl der Datensatz ego-Facebook weniger Knoten enthält als ca-GrQc, ist die relative Lösungsqualität besonders für $k \in \{4,6\}$ sehr hoch, das heißt, dass der Algorithmus dieser Arbeit vergleichsweise teure Partitionierungen ausgibt.
Der Hauptunterschied zwischen den beiden Datensätzen ist, dass ego-Facebook deutlich mehr Kanten und Dreiecke enthält.
Die hohe relative Lösungsqualität steht im Kontrast zu den Resultaten für Zufallsgraphen, da dort eine höhere Kantenanzahl zu einer geringen relativen Lösungsqualität führte.
Eine mögliche Ursache ist, dass das verwendete Kontraktionsverfahren für diesen Datensatz schlecht funktioniert.
Die Heuristiken verwenden ausgefeiltere Methoden, um den Graph zu kontrahieren.

In allen Experimenten ist bei der Verwendung von hierarchischer Dekomposition aufgefallen, dass die Performanz des Algorithmus dieser Arbeit für $k=2$ besser war als für $k \in \{2, 4, 6\}$.
Damit eignet sich der Algorithmus besonders für eine bikriterielle Approximation des Minimum-Bisection-Problems.
Eingangs wurde beschrieben, dass es zu diesem Problem bereits einige Approximationsalgorithmen gibt.
Mitunter gibt es einen bikriteriellen Approximationsalgorithmus mit Approximationsfaktor $\bigO(\sqrt{\log n}/\eps)$.~\cite{LR99, ARV09}
Dieser Algorithmus hat damit für alle praktikablen Werte von $\eps$ einen besseren Approximationsfaktor als der Algorithmus dieser Arbeit, der einen Approximationsfaktor von $\bigO(\log n)$ hat (siehe Sektion~\ref{sec:decomptrees}).
Erschwerend kommt hinzu, dass in der Implementierung eine hierarchische Dekomposition von Räcke, Shah und Täubig~\cite{RST14} mit einem Approximationsfaktor von $\bigO(\log^4 n)$ wurde. 
Zusätzlich kann für konstantes $k$ auch hierarchische Dekomposition in Kombination mit einem Algorithmus von Macgregor~\cite{mcg78} verwendet werden, der für $k=2$ nur eine Laufzeit von $\bigO(n^2)$ hat.

\section{Mögliche Verbesserungen}\label{sec:improv}
In dieser Sektion werden einige Möglichkeiten beschrieben, welche zu einer Verbesserung der Laufzeit des Algorithmus dieser Arbeit führen könnten.
Die Berechnung der Signaturen in der Schnittphase macht einen großen Anteil der Laufzeit des Algorithmus aus.
Für jeden Knoten wird dabei eine Menge an Signaturen berechnet, die sich im Wesentlichen durch die Kombination der Signaturen an den vorherigen Knoten ergibt.
In Sektion~\ref{sec:sigimpl} wurde beschrieben, dass eine Menge von Signaturen mit den optimalen Schnittkosten für jede Signatur als Hashtabelle gespeichert wird.
Dadurch kann effizient geprüft werden, ob eine Signatur bereits berechnet wurde und dementsprechend kann ein neuer Eintrag in der Tabelle erstellt werden beziehungsweise der bestehende Eintrag aktualisiert werden.
Wie in Listing~\ref{lst:hashing} zu sehen ist, verwendet die \Cpp{}\hyp Implementierung dieselbe Hashfunktion wie die weit verbeitete Softwarebibliothek Boost.

Da die Performanz der Hashtabelle hauptsächlich von der gewählten Hashfunktion abhängt, wäre es sinnvoll, alternative Hashfunktionen zu evaluieren.
Eine vielversprechende Alternative ist die Verwendung einer Cyclic\hyp Redundancy\hyp Checksumme (CRC), die sich in der Praxis als ein performante Hashfunktion herausstellt.~\cite{MPL09, Hua+11}
Außerdem ist CRC in einigen Prozessorarchitekturen~\cite{SSE} nativ implementiert, wodurch sich die Laufzeit des Hashings verbessern könnte.
Eine weitere Alternative wurde von Alcantara et al.~\cite{Alc+09} untersucht, die eine Implementierung einer Hashtabelle mit Grafikkartenbeschleunigung präsentierten.
Des Weiteren kann die Addition und Subtraktion von Signaturen auf der Grafikkarte durchgeführt werden.
Davon profitiert die Schnittphase und die Packphase, weil diese Operationen in beiden Phasen verwendet werden.

Zusätzlich zur Parallelisierung des Algorithmus durch Grafikkarten kann der Algorithmus folgendermaßen auf dem Prozessor parallelisiert werden:
Zur Berechnung der Signaturen an einem Knoten $v$ benötigen wir die Signaturen am linken Geschwisterknoten $w$ und am rechtesten Kind $u$ des Knotens.
Sei $T_x$ für einen Knoten $x$ des Baums der in $x$ gewurzelte Teilbaum.
Dann gilt, dass die Teilbäume $T_v$ und $T_w$ disjunkt sind.
Da $v$ ein Geschwisterknoten von $w$ ist, kann kein Knoten in $T_v$ -- außer $v$ -- einen linken Geschwisterknoten haben, der in $T_w$ liegt.
Deshalb können die Signaturen an den Kindern von $v$ parallel zu den Signaturen des linken Geschwisterknotens $w$ berechnet werden.

Es muss allerdings angemerkt werden, dass die obigen Vorschläge zwar das Potential haben die Berechnung der Signaturen zu beschleunigen, jedoch nicht die Anzahl der Signaturen verringern.
Da die Anzahl der Signaturen proportional zu $n$ und $k$ wächst, wird bei größeren Werten für diese Parameter nicht nur die Laufzeit, sondern auch der Speicherbedarf problematisch.
Es besteht die Möglichkeit die Anzahl der Signaturen zu verringern, indem der Graph zunächst mit einer Heuristik partitioniert wird und bei der Berechnung der Signaturen alle Signaturen frühzeitig herausgefiltert werden, deren Kosten höher sind als die durch die Heuristik berechneten Kosten.
Außerdem kann auf eine ähnliche Weise eine Binärsuche auf den Kosten durchgeführt werden.
Beide Vorschläge verringern jedoch nicht die Anzahl der Signaturen im schlimmsten Fall.
